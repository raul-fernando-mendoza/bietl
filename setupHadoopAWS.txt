##understanding hive version
https://docs.qubole.com/en/latest/user-guide/engines/hive/use-hive-versions.html

##create a hadoop user
sudo useradd -m -G sudo -U hadoop

#########using hadoop user add next in ~/.bashrc
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export PATH=$JAVA_HOME/bin:$PATH
export PDSH_RCMD_TYPE=ssh

export HADOOP_HOME=/opt/hadoop
export PATH=$HADOOP_HOME/bin:$PATH
export HIVE_HOME=/opt/hive
export PATH=$HIVE_HOME/bin:$PATH

export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"

#####################chage bash for hadoop
sudo chsh -s /bin/bash hadoop
##################### go to hadoop-env.sh and change the JAVA_HOME 
set JAVA_HOME=/opt/java
### run to setup ssh localhost with out password
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys
#### test with
ssh localhost
######################3 format the repository
bin/hdfs namenode -format
#################### start the hdfs
sbin/start-dfs.sh
################check status
ec2-52-15-229-150.us-east-2.compute.amazonaws.com:50070
################check server running
netstat -lpten | grep java
hadoop takes more than 2 GB of memory to make it work first make sure to have a swap file following instruction
https://linuxize.com/post/how-to-add-swap-space-on-ubuntu-18-04/


#### to add s3 support to hive add the following to hadoop-env.sh https://stackoverflow.com/questions/28029134/how-can-i-access-s3-s3n-from-a-local-hadoop-2-6-installation
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/tools/lib/*

####also add the following to coresite.xml retrive the keys from https://console.aws.amazon.com/iam/home#/security_credentials$access_key
<property>
  <name>fs.s3.impl</name>
  <value>org.apache.hadoop.fs.s3.S3FileSystem</value>
</property>

<property>
  <name>fs.s3n.impl</name>
  <value>org.apache.hadoop.fs.s3native.NativeS3FileSystem</value>
</property>

<property>
  <name>fs.s3n.awsAccessKeyId</name>
  <value>AKIAIFBSV7E3YFTSMX7A</value>
</property>

<property>
  <name>fs.s3n.awsSecretAccessKey</name>
  <value>t85pDkH8ShtKGVO29z5NZxI6jGQ5eM22wNp9vzlW</value>
</property>

<property>
  <name>fs.s3a.endpoint</name>
  <value>s3.us-east-2.amazonaws.com</value>
</property>
######################### HIVE create metastore

$HIVE_HOME/bin/schematool -dbType derby -initSchema

######################## HIVE init server

$HIVE_HOME/bin/hiveserver2
$HIVE_HOME/bin/hive --service hiveserver2

bin/hive --service hiveserver2 --stop
bin/hive --service metastore --stop




##############creating table in hive
CREATE EXTERNAL TABLE users
(
id int,
name string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n'
STORED AS TEXTFILE 
LOCATION 's3a://raul-folder/users';

CREATE EXTERNAL TABLE users3
(
id int,
name string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n'
STORED AS TEXTFILE 
LOCATION 's3a://raul-folder/users3';

insert into users3 select * from users;

export users department to 's3a://raul-folder/users4'

INSERT OVERWRITE DIRECTORY 's3a://raul-folder/output/'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
SELECT * FROM users;


insert into users values(1, 'raul');

hdfs dfs -Dfs.s3a.awsAccessKeyId=<access key ID> -Dfs.s3a.awsSecretAccessKey=<secret acces key> -Dfs.s3a.endpoint=<s3 enpoint> -ls s3a://<bucket_name>
hdfs dfs -Dfs.s3a.awsAccessKeyId=AKIAIFBSV7E3YFTSMX7A -Dfs.s3a.awsSecretAccessKey=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 -Dfs.s3a.endpoint=s3.us-east-2.amazonaws.com  -ls s3a://raul-folder/

hadoop fs -ls -Dfs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider -Dfs.s3a.endpoint=s3.us-east-2.amazonaws.com s3a://raul-public/

hadoop fs -ls s3a://raul-public/


#####MARIA  DB
****************Maria DB*****************


sudo apt install mariadb-server
sudo systemctl status mariadb

mysql -u root

create database entities;
use entities;

CREATE TABLE database_name( dummy VARCHAR(20) , version decimal(3,2));
insert into database_name values ('Entities',0.1);

drop user eApp@localhost;
CREATE USER eApp@localhost IDENTIFIED BY 'odroid';
GRANT ALL PRIVILEGES ON entities.database_name to eApp@localhost;
GRANT ALL PRIVILEGES ON *.* TO 'root'@localhost IDENTIFIED BY 'odroid' WITH GRANT OPTION;
GRANT ALL PRIVILEGES ON *.* TO 'eApp'@localhost IDENTIFIED BY 'odroid' WITH GRANT OPTION;

GRANT ALL PRIVILEGES ON *.* TO hadoop@localhost IDENTIFIED BY 'odroid' WITH GRANT OPTION;
GRANT ALL PRIVILEGES ON *.* TO root@localhost WITH GRANT OPTION;


GRANT ALL PRIVILEGES ON 'information_schema' TO 'root'@'localhost' IDENTIFIED BY 'odroid' WITH GRANT OPTION;

FLUSH PRIVILEGES;

####schema
/opt/hive/scripts/metastore/upgrade/mysql/hive-schema-2.3.0.mysql.sql

schematool -initSchema -dbType mysql

################## in hdfs-site.xml add
<property>
<name>dfs.namenode.resource.du.reserved</name>
<value>209715200</value>
</property>
######## run this before running client
export  HADOOP_CLIENT_OPTS=" -Xmx8192m"

##########3333 to add hadoop s3 support in hadoop-env add 
export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Dcom.amazonaws.services.s3.enableV4=true"
export HADOOP_OPTIONAL_TOOLS="hadoop-aws"


############# to fix PermGen Space problem use add -XX:PermSize=256M -XX:MaxPermSize=256M in hive_env.sh https://community.pivotal.io/s/article/Application-Server-Error-javalangOutOfMemoryError-PermGen-Space?language=en_US

export HADOOP_OPTS="$HADOOP_OPTS -XX:NewRatio=12 -Xms10m -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:-UseGCOverheadLimit -XX:PermSize=256M -XX:MaxPermSize=256M"

to be able to write to s3 load swa libraries

cp $HADOOP_HOME/share/hadoop/tools/lib/aws-java-sdk-1.7.4.jar $HIVE_HOME/lib/.
cp $HADOOP_HOME/share/hadoop/tools/lib/hadoop-aws-2.7.7.jar $HIVE_HOME/lib/

cp hadoop/share/hadoop/tools/lib/aws-java-sdk-1.7.4.jar hadoop/share/hadoop/common/lib/
cp hadoop/share/hadoop/tools/lib/hadoop-aws-2.7.7.jar hadoop/share/hadoop/common/lib/
cp hadoop/share/hadoop/tools/lib/jackson-databind-2.2.3.jar hadoop/share/hadoop/common/lib/
cp hadoop/share/hadoop/tools/lib/jackson-* hadoop/share/hadoop/common/lib/

 
